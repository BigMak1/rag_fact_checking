{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e94e8b-0671-40aa-9ea4-6444adbe5b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45953316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'rag_fact_checking'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 40, done.\u001b[K\n",
      "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
      "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
      "remote: Total 40 (delta 6), reused 40 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (40/40), 586.23 KiB | 5.75 MiB/s, done.\n",
      "Resolving deltas: 100% (6/6), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/BigMak1/rag_fact_checking.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097cbf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 4 root root 4096 Feb 26 08:40 .\n",
      "drwxr-xr-x 5 root root 4096 Feb 26 08:38 ..\n",
      "drwxr-xr-x 5 root root 4096 Feb 26 08:40 rag_fact_checking\n",
      "drwxr-xr-x 2 root root 4096 Feb 26 08:39 .virtual_documents\n"
     ]
    }
   ],
   "source": [
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "412f5693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f54f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.9/458.9 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.3/508.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.6/192.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.2/455.2 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m101.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.4/285.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.6/821.6 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.31.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "google-adk 1.21.0 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\n",
      "google-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.5 which is incompatible.\n",
      "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
      "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.5 which is incompatible.\n",
      "langgraph-prebuilt 1.0.6 requires langchain-core>=1.0.0, but you have langchain-core 0.3.83 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.5 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -r rag_fact_checking/DRAGON/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f6a9686-d441-4272-8d85-d663175c77dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"ERROR\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "from langchain_community.llms import VLLM\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# from rag_bench import baseline, data, evaluator, results\n",
    "from rag_fact_checking.DRAGON.rag_bench import baseline, data, evaluator, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "257e5c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58e063a2-2374-424b-80c7-a416084a4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST_PRIVATE_QA_REPO_ID: str = \"ai-forever/hist-rag-bench-private-qa\"\n",
    "HIST_PRIVATE_TEXTS_REPO_ID: str = \"ai-forever/hist-rag-bench-private-texts\"\n",
    "RANDOM_SEED: int = 42\n",
    "EMBEDDER_NAME: str = \"ai-forever/FRIDA\"\n",
    "LLM_NAME: str = \"bond005/meno-tiny-0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed67dad-5f51-422f-a429-9c04bc552ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проанализируйте заданный контекст и ответьте на вопрос пользователя на основе сведений, предоставленных в этом контексте.\n",
      "Не давайте никаких объяснений и пояснений к своему ответу. Не пишите ничего лишнего. Не извиняйтесь, не стройте диалог. Выдавайте только ответ и ничего больше.\n",
      "Отвечайте на русском языке.\n",
      "Если в заданном контексте нет информации для ответа на вопрос пользователя, то ничего не придумывайте и просто откажитесь отвечать.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LLM_PROMPT: str = \"\"\"Проанализируйте заданный контекст и ответьте на вопрос пользователя на основе сведений, предоставленных в этом контексте.\n",
    "Не давайте никаких объяснений и пояснений к своему ответу. Не пишите ничего лишнего. Не извиняйтесь, не стройте диалог. Выдавайте только ответ и ничего больше.\n",
    "Отвечайте на русском языке.\n",
    "Если в заданном контексте нет информации для ответа на вопрос пользователя, то ничего не придумывайте и просто откажитесь отвечать.\n",
    "\"\"\"\n",
    "print(LLM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a88d294f-2145-47cd-ae61-aec2473ce207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_private_qa_dataset(version):\n",
    "    return load_dataset(HIST_PRIVATE_QA_REPO_ID, revision=version)\n",
    "\n",
    "\n",
    "def get_private_texts_dataset(version):\n",
    "    return load_dataset(HIST_PRIVATE_TEXTS_REPO_ID, revision=version)\n",
    "\n",
    "\n",
    "def get_public_to_private_texts_mapping(version):\n",
    "    private_texts_ds = get_private_texts_dataset(version)\n",
    "    mapping = {}\n",
    "    for item in private_texts_ds[\"train\"]:\n",
    "        mapping[item[\"public_id\"]] = item[\"id\"]\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36e62795-dae3-4957-88de-ffa298df6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.random.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "917c62ac-5b26-4c61-8d08-67ae2ef72f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest texts version: 1.15.0\n",
      "Latest questions version: 1.15.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868d431ed9314239adcee8d033992244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/374 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e231ce051f44e6bb56a6651789a4e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/793k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410868031c2c4bef9425425786ed84e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3b5088e69740bdaf4c285f72c535d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/377 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90f8da811e242e29bf94667f5d99397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/40.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec12685139043bb9e026104c9ebbfa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded texts dataset with 542 texts\n",
      "Loaded questions dataset with 600 questions\n",
      "version = 1.15.0\n"
     ]
    }
   ],
   "source": [
    "#get public datasets (history ones)\n",
    "texts_ds, questions_ds, version = data.get_datasets(is_hist=True)\n",
    "print(f\"version = {version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d480095-21b4-4849-bc6e-da5d6d276647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f062bb16d643ddbb064c5993430bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/591 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc78a2491c394543b5b29c7bad11ce26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/646k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27f368ea53440988375e52b52db7a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26c2f9acd944d1c8c50fde45c34b86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/423 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc04471893f49c08b8a2201b64a3faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d811aaf69a41445994a72f2c75ea79a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get private datasets (history ones)\n",
    "qa_dataset = get_private_qa_dataset(version)\n",
    "mapping = get_public_to_private_texts_mapping(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d4cba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIBRARY_PATH = /usr/local/nvidia/lib64:/usr/local/cuda/lib64/stubs\n"
     ]
    }
   ],
   "source": [
    "# Kaggle FIX\n",
    "import os                                                                                                                                              \n",
    "# Добавляем путь с libcuda.so в LIBRARY_PATH (используется линкером при JIT-компиляции)                                                                \n",
    "os.environ[\"LIBRARY_PATH\"] = \"/usr/local/nvidia/lib64:\" + os.environ.get(\"LIBRARY_PATH\", \"\")                                                           \n",
    "print(\"LIBRARY_PATH =\", os.environ[\"LIBRARY_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4fcddd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 08:45:42.290229: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1772095542.509697     101 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1772095542.567307     101 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1772095543.042833     101 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772095543.042869     101 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772095543.042872     101 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772095543.042874     101 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c9c4d071f449fdab8d42217c0594f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/717 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d226e99c5b4d89aee2d8f9d666e4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef4fb22905540fd9af5bd954aa3869b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6265c1fbbc5c49338e14d6b91cb3e02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9392707ec3b54043872f59b4ac284082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8837db42e64bf0a9e9ff776fe914a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59a41bc286342cab7cbb3eeeb93b32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfd0b995eb4433d8759471a46bf7100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/499 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 08:46:26.676153: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1772095586.703205     276 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1772095586.713475     276 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1772095586.743834     276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772095586.743878     276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772095586.743884     276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772095586.743889     276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "2026-02-26 08:46:41.348362: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-26 08:46:41.349043: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1772095601.373618     301 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1772095601.378535     302 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1772095601.381259     301 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "E0000 00:00:1772095601.386279     302 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1772095601.399390     301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772095601.399423     301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772095601.399427     301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772095601.399429     301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772095601.403673     302 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772095601.403704     302 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772095601.403707     302 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772095601.403709     302 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n",
      "AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Worker_TP1 pid=302) ERROR 02-26 08:46:55 [fa_utils.py:104] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "(Worker_TP0 pid=301) ERROR 02-26 08:46:55 [fa_utils.py:104] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Worker_TP0 pid=301) <frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "(Worker_TP0 pid=301) <frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "(Worker_TP1 pid=302) <frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "(Worker_TP1 pid=302) <frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.78s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.78s/it]\n",
      "(Worker_TP0 pid=301) \n",
      "(Worker_TP1 pid=302) [rank1]:W0226 08:48:00.369000 302 torch/_inductor/utils.py:1613] Not enough SMs to use max_autotune_gemm mode\n",
      "(Worker_TP0 pid=301) [rank0]:W0226 08:48:00.888000 301 torch/_inductor/utils.py:1613] Not enough SMs to use max_autotune_gemm mode\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:04<00:00, 11.88it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 11.84it/s]\n"
     ]
    }
   ],
   "source": [
    "llm = VLLM(\n",
    "    model=LLM_NAME,\n",
    "    tensor_parallel_size=2,  # используем оба GPU\n",
    "    max_new_tokens=256,\n",
    "    top_p=0.95,\n",
    "    temperature=0.3,\n",
    "    vllm_kwargs={\n",
    "        \"gpu_memory_utilization\": 0.45,\n",
    "        \"max_num_batched_tokens\": 8192,\n",
    "        \"max_model_len\": 4096,\n",
    "        \"disable_log_stats\": True,\n",
    "        \"seed\": RANDOM_SEED\n",
    "    },\n",
    "    disable_log_stats=True,\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(LLM_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec0884fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VLLM(model='bond005/meno-tiny-0.1', tensor_parallel_size=2, temperature=0.3, top_p=0.95, max_new_tokens=256, vllm_kwargs={'gpu_memory_utilization': 0.45, 'max_num_batched_tokens': 8192, 'max_model_len': 4096, 'disable_log_stats': True, 'seed': 42}, client=Qwen2ForCausalLM()(\n",
       "  (model): Qwen2Model()(\n",
       "    (embed_tokens): VocabParallelEmbedding(num_embeddings=75968, embedding_dim=1536, org_vocab_size=151936, num_embeddings_padded=151936, tp_size=2)\n",
       "    (layers): ModuleList()(\n",
       "      (0-27): 28 x Qwen2DecoderLayer()(\n",
       "        (self_attn): Qwen2Attention()(\n",
       "          (qkv_proj): QKVParallelLinear(in_features=1536, output_features=1024, bias=True, tp_size=2, gather_output=False)\n",
       "          (o_proj): RowParallelLinear(in_features=768, output_features=1536, bias=False, tp_size=2, reduce_results=True)\n",
       "          (rotary_emb): RotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=32768, base=1000000.0, is_neox_style=True)(\n",
       "            (apply_rotary_emb): ApplyRotaryEmb(is_neox_style=True, enable_fp32_compute=False)\n",
       "          )\n",
       "          (attn): Attention(head_size=128, num_heads=6, num_kv_heads=1, scale=0.08838834764831845, backend=FlashInferImpl)\n",
       "        )\n",
       "        (mlp): Qwen2MLP()(\n",
       "          (gate_up_proj): MergedColumnParallelLinear(in_features=1536, output_features=8960, bias=False, tp_size=2, gather_output=False)\n",
       "          (down_proj): RowParallelLinear(in_features=4480, output_features=1536, bias=False, tp_size=2, reduce_results=True)\n",
       "          (act_fn): SiluAndMul()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm(hidden_size=1536, eps=1e-06)\n",
       "        (post_attention_layernorm): RMSNorm(hidden_size=1536, eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm(hidden_size=1536, eps=1e-06)\n",
       "  )\n",
       "  (lm_head): VocabParallelEmbedding(num_embeddings=75968, embedding_dim=1536, org_vocab_size=151936, num_embeddings_padded=151936, tp_size=2)\n",
       "  (logits_processor): LogitsProcessor(vocab_size=151936, org_vocab_size=151936, scale=1.0, logits_as_input=False)\n",
       "))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36ef2c3e-24b6-4f69-b832-76eacd2dab6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80ac2817a4442b4b015a044026217d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de40d69513cf48d2ac7bd3914296c2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/509 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075f7f29525b40779b03dc3b9134387d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2ef6c182244234adde06f01d1eccd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ef6411ffbb40088a658efe8497fa7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/823 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb12f06c235a4ab49b67ed6f8e0960fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.29G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba2c264a6bd4ee1ba062fb8b54a05af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90061b8b1034b09b2a7d2c4d2a5926f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b56aa06321c46bfa2f3beff47093a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8fbb35e5184608aaec7316e82fbc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573d7e718ef94bd4b50e7dc7a62c9634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/958 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c072325abd4752804b7defca723f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"ai-forever/FRIDA\",\n",
    "    model_kwargs={\"trust_remote_code\": True},\n",
    "    encode_kwargs={\"batch_size\": 16, \"prompt\": \"search_document: \"},\n",
    "    query_encode_kwargs={\"prompt\": \"search_query: \"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc32a71f-bdf4-46b7-8a58-7d0c36e49dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing retriever\n",
      "> Creating vector store\n",
      "> Vector store created for 542 documents\n",
      "> Done for 125.52 seconds\n",
      "Initializing generation chain\n",
      "> Done\n"
     ]
    }
   ],
   "source": [
    "# HYBRID_RERANK = False\n",
    "# RERANK_CANDIDATE_K = 20\n",
    "\n",
    "retrieval = baseline.init_retriever(\n",
    "    texts_ds,\n",
    "    embedding_model,\n",
    "    top_k=5,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    # hybrid_rerank=HYBRID_RERANK,\n",
    "    # rerank_candidate_k=RERANK_CANDIDATE_K,\n",
    ")\n",
    "generation = baseline.init_generation(retrieval, llm, tok, system_prompt=LLM_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b660e73-c109-4997-8dc4-80d90aaa9372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.9 s, sys: 3.04 s, total: 59.9 s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "res = baseline.get_results(\n",
    "    generation, questions_ds, write_logs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f826431c-1b99-4277-ad6a-8829bbae04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the baseline results\n",
    "\n",
    "res_path = \"./test.json\"\n",
    "results.save(res, res_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37bbaa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 152\n",
      "drwxr-xr-x 4 root root   4096 Feb 26 08:57 .\n",
      "drwxr-xr-x 5 root root   4096 Feb 26 08:38 ..\n",
      "-rw-r--r-- 1 root root    579 Feb 26 08:57 log.txt\n",
      "drwxr-xr-x 5 root root   4096 Feb 26 08:40 rag_fact_checking\n",
      "-rw-r--r-- 1 root root 131818 Feb 26 08:57 test.json\n",
      "drwxr-xr-x 2 root root   4096 Feb 26 08:39 .virtual_documents\n"
     ]
    }
   ],
   "source": [
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9926db5-24ec-44b9-9e2c-cb10a38ae757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd10f5180fd24cc1b263deafb54ee297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'question': 'Какой регион был определён в качестве приоритетного для Дональда Трампа?'}\n",
      "\n",
      "{\n",
      "    \"found_ids\": [\n",
      "        197,\n",
      "        134,\n",
      "        399,\n",
      "        115,\n",
      "        458\n",
      "    ],\n",
      "    \"model_answer\": \"Индхо-Тихоокеанский регион\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "demo_public_id = 0\n",
    "print(questions_ds[\"train\"].filter(lambda it: it[\"id\"] == demo_public_id)[0])\n",
    "print('\\n' + json.dumps(res[demo_public_id], ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6063a5ed-d73d-44cf-aac7-f78486f6e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate metrics\n",
    "\n",
    "evaluation_results = evaluator.evaluate_rag_results(res, qa_dataset, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f7f5f3a-ed7a-4abf-ab3b-3f64edbfdac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Metrics:\n",
      "+----------+---------+\n",
      "| Metric   |   Value |\n",
      "+==========+=========+\n",
      "| Hit Rate |  0.7997 |\n",
      "+----------+---------+\n",
      "| MRR      |  0.7893 |\n",
      "+----------+---------+\n",
      "\n",
      "Generation Metrics:\n",
      "+-----------------+---------+\n",
      "| Metric          |   Value |\n",
      "+=================+=========+\n",
      "| ROUGE-1         |  0.5034 |\n",
      "+-----------------+---------+\n",
      "| ROUGE-2         |  0.3143 |\n",
      "+-----------------+---------+\n",
      "| ROUGE-L         |  0.4809 |\n",
      "+-----------------+---------+\n",
      "| Exact Match     |  0.1250 |\n",
      "+-----------------+---------+\n",
      "| Substring Match |  0.1483 |\n",
      "+-----------------+---------+\n"
     ]
    }
   ],
   "source": [
    "_ = evaluation_results.to_table(overall_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "199d52cd-fa44-4250-9457-dcab96322a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Metrics:\n",
      "+----------+-----------+--------+--------+--------+----------+\n",
      "| Metric   |   Overall |   cond |     mh |    set |   simple |\n",
      "+==========+===========+========+========+========+==========+\n",
      "| Hit Rate |    0.7997 | 0.7733 | 0.7756 | 0.8033 |   0.8467 |\n",
      "+----------+-----------+--------+--------+--------+----------+\n",
      "| MRR      |    0.7893 | 0.7999 | 0.7730 | 0.7841 |   0.8000 |\n",
      "+----------+-----------+--------+--------+--------+----------+\n",
      "\n",
      "Generation Metrics:\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| Metric          |   Overall |   cond |     mh |    set |   simple |\n",
      "+=================+===========+========+========+========+==========+\n",
      "| ROUGE-1         |    0.5034 | 0.6111 | 0.4798 | 0.4259 |   0.4968 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| ROUGE-2         |    0.3143 | 0.3670 | 0.2860 | 0.2265 |   0.3778 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| ROUGE-L         |    0.4809 | 0.6077 | 0.4782 | 0.3411 |   0.4968 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| Exact Match     |    0.1250 | 0.2467 | 0.1267 | 0.0000 |   0.1267 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| Substring Match |    0.1483 | 0.2800 | 0.1267 | 0.0200 |   0.1667 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "_ = evaluation_results.to_table(overall_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8fa311",
   "metadata": {},
   "source": [
    "## Top-10 Documents By Fact-Check Score\n",
    "Вычисляет score класса `true` (DeReC verifier) для каждого ответа и агрегирует его по найденным документам (`found_ids`).\n",
    "Выводит top-10 документов по среднему score и количеству появлений.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f0481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "TOP_N_DOCS = 10\n",
    "\n",
    "if not (DEREC_VERIFIER_REPO and DEREC_VERIFIER_REPO.strip()):\n",
    "    print(\"Set DEREC_VERIFIER_REPO first (cell 'DeReC Verifier (optional)').\")\n",
    "else:\n",
    "    verifier_tok = AutoTokenizer.from_pretrained(DEREC_VERIFIER_REPO, token=(HF_TOKEN.strip() or None))\n",
    "    verifier = AutoModelForSequenceClassification.from_pretrained(DEREC_VERIFIER_REPO, token=(HF_TOKEN.strip() or None))\n",
    "    verifier.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    verifier.to(device)\n",
    "\n",
    "    q_idx = _build_question_index(questions_ds)\n",
    "    t_idx = _build_text_index(texts_ds)\n",
    "\n",
    "    doc_score_sum = defaultdict(float)\n",
    "    doc_score_cnt = defaultdict(int)\n",
    "    doc_score_max = defaultdict(float)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for qid, pred in res.items():\n",
    "            qid_str = str(qid)\n",
    "            question = q_idx.get(qid_str, {}).get(\"question\", \"\")\n",
    "            answer = pred.get(\"model_answer\", \"\")\n",
    "            found_ids = pred.get(\"found_ids\", [])\n",
    "            evidence = \"\\n\".join([t_idx.get(doc_id, \"\") for doc_id in found_ids])[:4000]\n",
    "\n",
    "            text = f\"claim: question: {question}\\nanswer: {answer} [SEP] evidence: {evidence}\"\n",
    "            inp = verifier_tok(text, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "            inp = {k: v.to(device) for k, v in inp.items()}\n",
    "            probs = torch.softmax(verifier(**inp).logits, dim=-1)[0].cpu().numpy()\n",
    "            true_score = float(probs[2] if probs.shape[0] >= 3 else probs.max())\n",
    "\n",
    "            for doc_id in found_ids:\n",
    "                doc_score_sum[doc_id] += true_score\n",
    "                doc_score_cnt[doc_id] += 1\n",
    "                doc_score_max[doc_id] = max(doc_score_max[doc_id], true_score)\n",
    "\n",
    "    ranked = []\n",
    "    for doc_id, total in doc_score_sum.items():\n",
    "        cnt = doc_score_cnt[doc_id]\n",
    "        avg = total / cnt\n",
    "        ranked.append((doc_id, avg, cnt, doc_score_max[doc_id]))\n",
    "\n",
    "    ranked.sort(key=lambda x: (-x[1], -x[2], -x[3]))\n",
    "    top_docs = ranked[:TOP_N_DOCS]\n",
    "\n",
    "    print(f\"Top-{TOP_N_DOCS} documents by average fact-check true_score\")\n",
    "    for i, (doc_id, avg_score, cnt, max_score) in enumerate(top_docs, 1):\n",
    "        print(f\"{i:02d}. doc_id={doc_id} | avg_score={avg_score:.4f} | occurrences={cnt} | max_score={max_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
