{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e94e8b-0671-40aa-9ea4-6444adbe5b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45953316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DRAGON'...\n",
      "remote: Enumerating objects: 463, done.\u001b[K\n",
      "remote: Counting objects: 100% (230/230), done.\u001b[K\n",
      "remote: Compressing objects: 100% (139/139), done.\u001b[K\n",
      "remote: Total 463 (delta 110), reused 178 (delta 80), pack-reused 233 (from 1)\u001b[K\n",
      "Receiving objects: 100% (463/463), 42.16 MiB | 38.83 MiB/s, done.\n",
      "Resolving deltas: 100% (193/193), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/RussianNLP/DRAGON.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097cbf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12\n",
      "drwxr-xr-x 3 root root 4096 Feb 25 18:51 .\n",
      "drwxr-xr-x 5 root root 4096 Feb 25 18:51 ..\n",
      "drwxr-xr-x 2 root root 4096 Feb 25 18:51 .virtual_documents\n"
     ]
    }
   ],
   "source": [
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "412f5693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9878c1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.9/458.9 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.6/192.6 kB\u001b[0m \u001b[31m98.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m45.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m333.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.2/455.2 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.4/285.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.6/821.6 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.31.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "google-adk 1.21.0 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\n",
      "google-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.5 which is incompatible.\n",
      "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
      "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.5 which is incompatible.\n",
      "langgraph-prebuilt 1.0.6 requires langchain-core>=1.0.0, but you have langchain-core 0.3.83 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.5 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q \"datasets\" \"langchain<1.0\" \"langchain-community<0.4\" \"langchain-core<1.0\" \"langchain-huggingface<1.0\" \"numpy\" \"tqdm\" \"torch>=2.1.0\" \"transformers\" \"vllm>=0.7.3\" \"langchain_chroma<1.0\" \"rouge_score\" \"razdel\" \"rag-bench>=0.2.12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f6a9686-d441-4272-8d85-d663175c77dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"ERROR\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "from langchain_community.llms import VLLM\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# from rag_bench import baseline, data, evaluator, results\n",
    "from DRAGON.lib.src.rag_bench import baseline, data, evaluator, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "257e5c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58e063a2-2374-424b-80c7-a416084a4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST_PRIVATE_QA_REPO_ID: str = \"ai-forever/hist-rag-bench-private-qa\"\n",
    "HIST_PRIVATE_TEXTS_REPO_ID: str = \"ai-forever/hist-rag-bench-private-texts\"\n",
    "RANDOM_SEED: int = 42\n",
    "EMBEDDER_NAME: str = \"ai-forever/FRIDA\"\n",
    "LLM_NAME: str = \"bond005/meno-tiny-0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ed67dad-5f51-422f-a429-9c04bc552ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проанализируйте заданный контекст и ответьте на вопрос пользователя на основе сведений, предоставленных в этом контексте.\n",
      "Не давайте никаких объяснений и пояснений к своему ответу. Не пишите ничего лишнего. Не извиняйтесь, не стройте диалог. Выдавайте только ответ и ничего больше.\n",
      "Отвечайте на русском языке.\n",
      "Если в заданном контексте нет информации для ответа на вопрос пользователя, то ничего не придумывайте и просто откажитесь отвечать.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LLM_PROMPT: str = \"\"\"Проанализируйте заданный контекст и ответьте на вопрос пользователя на основе сведений, предоставленных в этом контексте.\n",
    "Не давайте никаких объяснений и пояснений к своему ответу. Не пишите ничего лишнего. Не извиняйтесь, не стройте диалог. Выдавайте только ответ и ничего больше.\n",
    "Отвечайте на русском языке.\n",
    "Если в заданном контексте нет информации для ответа на вопрос пользователя, то ничего не придумывайте и просто откажитесь отвечать.\n",
    "\"\"\"\n",
    "print(LLM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a88d294f-2145-47cd-ae61-aec2473ce207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_private_qa_dataset(version):\n",
    "    return load_dataset(HIST_PRIVATE_QA_REPO_ID, revision=version)\n",
    "\n",
    "\n",
    "def get_private_texts_dataset(version):\n",
    "    return load_dataset(HIST_PRIVATE_TEXTS_REPO_ID, revision=version)\n",
    "\n",
    "\n",
    "def get_public_to_private_texts_mapping(version):\n",
    "    private_texts_ds = get_private_texts_dataset(version)\n",
    "    mapping = {}\n",
    "    for item in private_texts_ds[\"train\"]:\n",
    "        mapping[item[\"public_id\"]] = item[\"id\"]\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36e62795-dae3-4957-88de-ffa298df6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.random.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "917c62ac-5b26-4c61-8d08-67ae2ef72f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest texts version: 1.15.0\n",
      "Latest questions version: 1.15.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a52cdc4815496883cc6cf961e3935e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/374 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbfb0428f6746119d1a9248727bbf94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/793k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143c1842b67c43868fd8dad859e38a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb70874c1bb417383d7543eab85efec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/377 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd9a9b6cd58432a9c7d4b6b21c59ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/40.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e2661622d74d9abd95703abee92b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded texts dataset with 542 texts\n",
      "Loaded questions dataset with 600 questions\n",
      "version = 1.15.0\n"
     ]
    }
   ],
   "source": [
    "#get public datasets (history ones)\n",
    "texts_ds, questions_ds, version = data.get_datasets(is_hist=True)\n",
    "print(f\"version = {version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d480095-21b4-4849-bc6e-da5d6d276647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06bada2b8444f9687c18ec1e23fd667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/591 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3200b73941544aa3a84ca89c56ed65d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/646k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7826d9452c9e44c0a47e548dc3ad09b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bb64fb7bfc4a1595a103097d3af962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/423 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8993b9410ed47cc80388ddd8c792025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbb672718624e2fa7c92898a682358d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get private datasets (history ones)\n",
    "qa_dataset = get_private_qa_dataset(version)\n",
    "mapping = get_public_to_private_texts_mapping(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4cba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIBRARY_PATH = /usr/local/nvidia/lib64:/usr/local/cuda/lib64/stubs\n"
     ]
    }
   ],
   "source": [
    "# Kaggle FIX\n",
    "import os                                                                                                                                              \n",
    "# Добавляем путь с libcuda.so в LIBRARY_PATH (используется линкером при JIT-компиляции)                                                                \n",
    "os.environ[\"LIBRARY_PATH\"] = \"/usr/local/nvidia/lib64:\" + os.environ.get(\"LIBRARY_PATH\", \"\")                                                           \n",
    "print(\"LIBRARY_PATH =\", os.environ[\"LIBRARY_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fcddd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 19:25:05.714408: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1772047505.742555    1170 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1772047505.750521    1170 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1772047505.771225    1170 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772047505.771256    1170 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772047505.771259    1170 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1772047505.771262    1170 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "[W225 19:25:18.440605428 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=1170)\u001b[0;0m ERROR 02-25 19:25:19 [fa_utils.py:86] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=1170)\u001b[0;0m <frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1170)\u001b[0;0m <frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.nvrtc module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.nvrtc module instead.\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.89s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.89s/it]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=1170)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:03<00:00, 13.97it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 13.66it/s]\n"
     ]
    }
   ],
   "source": [
    "llm = VLLM(\n",
    "    model=LLM_NAME,\n",
    "    tensor_parallel_size=1,  # используем оба GPU\n",
    "    max_new_tokens=256,\n",
    "    top_p=0.95,\n",
    "    temperature=0.3,\n",
    "    vllm_kwargs={\n",
    "        \"gpu_memory_utilization\": 0.45,\n",
    "        \"max_num_batched_tokens\": 8192,\n",
    "        \"max_model_len\": 4096,\n",
    "        \"disable_log_stats\": True,\n",
    "        \"seed\": RANDOM_SEED\n",
    "    },\n",
    "    disable_log_stats=True,\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(LLM_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec0884fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VLLM(model='bond005/meno-tiny-0.1', temperature=0.3, top_p=0.95, max_new_tokens=256, vllm_kwargs={'gpu_memory_utilization': 0.45, 'max_num_batched_tokens': 8192, 'max_model_len': 4096, 'disable_log_stats': True, 'seed': 42}, client=Qwen2ForCausalLM()(\n",
       "  (model): Qwen2Model()(\n",
       "    (embed_tokens): VocabParallelEmbedding(num_embeddings=151936, embedding_dim=1536, org_vocab_size=151936, num_embeddings_padded=151936, tp_size=1)\n",
       "    (layers): ModuleList()(\n",
       "      (0-27): 28 x Qwen2DecoderLayer()(\n",
       "        (self_attn): Qwen2Attention()(\n",
       "          (qkv_proj): QKVParallelLinear(in_features=1536, output_features=2048, bias=True, tp_size=1, gather_output=False)\n",
       "          (o_proj): RowParallelLinear(in_features=1536, output_features=1536, bias=False, tp_size=1, reduce_results=True)\n",
       "          (rotary_emb): RotaryEmbedding(head_size=128, rotary_dim=128, max_position_embeddings=32768, base=1000000.0, is_neox_style=True)(\n",
       "            (apply_rotary_emb): ApplyRotaryEmb(is_neox_style=True, enable_fp32_compute=False)\n",
       "          )\n",
       "          (attn): Attention(head_size=128, num_heads=12, num_kv_heads=2, scale=0.08838834764831845, backend=FlashInferImpl)\n",
       "        )\n",
       "        (mlp): Qwen2MLP()(\n",
       "          (gate_up_proj): MergedColumnParallelLinear(in_features=1536, output_features=17920, bias=False, tp_size=1, gather_output=False)\n",
       "          (down_proj): RowParallelLinear(in_features=8960, output_features=1536, bias=False, tp_size=1, reduce_results=True)\n",
       "          (act_fn): SiluAndMul()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm(hidden_size=1536, eps=1e-06)\n",
       "        (post_attention_layernorm): RMSNorm(hidden_size=1536, eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm(hidden_size=1536, eps=1e-06)\n",
       "  )\n",
       "  (lm_head): VocabParallelEmbedding(num_embeddings=151936, embedding_dim=1536, org_vocab_size=151936, num_embeddings_padded=151936, tp_size=1)\n",
       "  (logits_processor): LogitsProcessor(vocab_size=151936, org_vocab_size=151936, scale=1.0, logits_as_input=False)\n",
       "))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36ef2c3e-24b6-4f69-b832-76eacd2dab6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cd8855919a4bb2805b98891d85b0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e312e4a948d84a2ca1c7ca3488ec421b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/509 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893b888a655d4dc5b7780469d29a7fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad143f8b832469f9e425adc05f3260d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587ed5dce8ac493eb91866f086d84ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/823 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747c96dd5fa84f8889543a6be5b52f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.29G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb0e163308b47e6859701d0bbaaf1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905f01730ff14607bbba25e93bd9308f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef234444fc1475badcbbf9c632a6509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3381e081e1974950afc6c382c57f95c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69a0dc6a6114765b6dc2d937547c46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/958 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7804170147b14894ae7c61a43575945c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"ai-forever/FRIDA\",\n",
    "    model_kwargs={\"trust_remote_code\": True},\n",
    "    encode_kwargs={\"batch_size\": 16, \"prompt\": \"search_document: \"},\n",
    "    query_encode_kwargs={\"prompt\": \"search_query: \"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc32a71f-bdf4-46b7-8a58-7d0c36e49dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing retriever\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Creating vector store\n",
      "> Vector store created for 542 documents\n",
      "> Done for 117.34 seconds\n",
      "Initializing generation chain\n",
      "> Done\n"
     ]
    }
   ],
   "source": [
    "# HYBRID_RERANK = False\n",
    "# RERANK_CANDIDATE_K = 20\n",
    "\n",
    "retrieval = baseline.init_retriever(\n",
    "    texts_ds,\n",
    "    embedding_model,\n",
    "    top_k=5,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    # hybrid_rerank=HYBRID_RERANK,\n",
    "    # rerank_candidate_k=RERANK_CANDIDATE_K,\n",
    ")\n",
    "generation = baseline.init_generation(retrieval, llm, tok, system_prompt=LLM_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b660e73-c109-4997-8dc4-80d90aaa9372",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = baseline.get_results(\n",
    "    generation, questions_ds, write_logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f826431c-1b99-4277-ad6a-8829bbae04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the baseline results\n",
    "\n",
    "res_path = \"./test.json\"\n",
    "results.save(res, res_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9926db5-24ec-44b9-9e2c-cb10a38ae757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8e8a6819434c5981c436a4d5711a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'question': 'Какой регион был определён в качестве приоритетного для Дональда Трампа?'}\n",
      "\n",
      "{\n",
      "    \"found_ids\": [\n",
      "        197,\n",
      "        134,\n",
      "        399,\n",
      "        115,\n",
      "        458\n",
      "    ],\n",
      "    \"model_answer\": \"Индхо-Тихоокеанский регион\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "demo_public_id = 0\n",
    "print(questions_ds[\"train\"].filter(lambda it: it[\"id\"] == demo_public_id)[0])\n",
    "print('\\n' + json.dumps(res[demo_public_id], ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063a5ed-d73d-44cf-aac7-f78486f6e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate metrics\n",
    "\n",
    "evaluation_results = evaluator.evaluate_rag_results(res, qa_dataset, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f5f3a-ed7a-4abf-ab3b-3f64edbfdac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Metrics:\n",
      "+----------+---------+\n",
      "| Metric   |   Value |\n",
      "+==========+=========+\n",
      "| Hit Rate |  0.7997 |\n",
      "+----------+---------+\n",
      "| MRR      |  0.7893 |\n",
      "+----------+---------+\n",
      "\n",
      "Generation Metrics:\n",
      "+-----------------+---------+\n",
      "| Metric          |   Value |\n",
      "+=================+=========+\n",
      "| ROUGE-1         |  0.4997 |\n",
      "+-----------------+---------+\n",
      "| ROUGE-2         |  0.3103 |\n",
      "+-----------------+---------+\n",
      "| ROUGE-L         |  0.4773 |\n",
      "+-----------------+---------+\n",
      "| Exact Match     |  0.1250 |\n",
      "+-----------------+---------+\n",
      "| Substring Match |  0.1467 |\n",
      "+-----------------+---------+\n"
     ]
    }
   ],
   "source": [
    "_ = evaluation_results.to_table(overall_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d52cd-fa44-4250-9457-dcab96322a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Metrics:\n",
      "+----------+-----------+--------+--------+--------+----------+\n",
      "| Metric   |   Overall |   cond |     mh |    set |   simple |\n",
      "+==========+===========+========+========+========+==========+\n",
      "| Hit Rate |    0.7997 | 0.7733 | 0.7756 | 0.8033 |   0.8467 |\n",
      "+----------+-----------+--------+--------+--------+----------+\n",
      "| MRR      |    0.7893 | 0.7999 | 0.7730 | 0.7841 |   0.8000 |\n",
      "+----------+-----------+--------+--------+--------+----------+\n",
      "\n",
      "Generation Metrics:\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| Metric          |   Overall |   cond |     mh |    set |   simple |\n",
      "+=================+===========+========+========+========+==========+\n",
      "| ROUGE-1         |    0.4997 | 0.6067 | 0.4938 | 0.4180 |   0.4804 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| ROUGE-2         |    0.3103 | 0.3708 | 0.2920 | 0.2126 |   0.3656 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| ROUGE-L         |    0.4773 | 0.6034 | 0.4929 | 0.3325 |   0.4804 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| Exact Match     |    0.1250 | 0.2467 | 0.1067 | 0.0000 |   0.1467 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| Substring Match |    0.1467 | 0.2800 | 0.1067 | 0.0133 |   0.1867 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "_ = evaluation_results.to_table(overall_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d160b4",
   "metadata": {},
   "source": [
    "## DeReC Verifier (optional)\n",
    "Трехзонная логика поверх confidence verifier:\n",
    "- `score < LOW` -> `abstain`\n",
    "- `LOW <= score < HIGH` -> `rewrite/keep/abstain` (настраивается)\n",
    "- `score >= HIGH` -> `keep`\n",
    "\n",
    "Есть опция калибровки `LOW/HIGH` на dev-срезе DRAGON по выбранной метрике.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2671632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# USE_DEREC_VERIFIER = True\n",
    "# DEREC_VERIFIER_REPO = \"Makson4ic/derec-rawfc-deberta-frida\"  # например: \"username/derec-rawfc-deberta-frida\"\n",
    "# HF_TOKEN = \"\"  # для private repo\n",
    "\n",
    "# DEREC_LOW_THRESHOLD = 0.15\n",
    "# DEREC_HIGH_THRESHOLD = 0.3\n",
    "# DEREC_MEDIUM_ACTION = \"rewrite\"  # \"rewrite\" | \"keep\" | \"abstain\"\n",
    "# DEREC_CALIBRATE = True\n",
    "# DEREC_DEV_SIZE = 100\n",
    "# DEREC_CALIBRATION_METRIC = \"exact_match\"  # exact_match | substring_match | rougeL\n",
    "# ABSTAIN_TEXT = \"не хватает данных в контексте\"\n",
    "\n",
    "\n",
    "# def _build_question_index(questions_ds):\n",
    "#     return {str(item[\"id\"]): item for item in questions_ds[\"train\"]}\n",
    "\n",
    "\n",
    "# def _build_text_index(texts_ds):\n",
    "#     return {item[\"id\"]: item[\"text\"] for item in texts_ds[\"train\"]}\n",
    "\n",
    "\n",
    "# def _build_answer_map(qa_dataset):\n",
    "#     return {str(item[\"public_id\"]): item[\"answer\"] for item in qa_dataset[\"train\"]}\n",
    "\n",
    "\n",
    "# def _build_rewrite_prompt(tokenizer, question, evidence, answer):\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": \"Перепиши ответ так, чтобы остались только факты, подтверждаемые контекстом. Если контекста недостаточно, верни точную фразу: не хватает данных в контексте.\",\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": (\n",
    "#                 f\"Контекст:\\n{evidence}\\n\\n\"\n",
    "#                 f\"Вопрос:\\n{question}\\n\\n\"\n",
    "#                 f\"Исходный ответ:\\n{answer}\\n\\n\"\n",
    "#                 \"Сформулируй финальный ответ одной короткой строкой.\"\n",
    "#             ),\n",
    "#         },\n",
    "#     ]\n",
    "#     return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "# def _rewrite_answer(question, evidence, answer):\n",
    "#     return llm.invoke(_build_rewrite_prompt(tok, question, evidence, answer)).strip()\n",
    "\n",
    "\n",
    "# def _score_answer(metric_name, generated, reference):\n",
    "#     e = evaluator.RAGEvaluator()\n",
    "#     scores = e.evaluate_generation(generated_answer=generated, reference_answer=reference)\n",
    "#     return float(scores[metric_name])\n",
    "\n",
    "\n",
    "# def _choose_answer(keep_answer, rewritten_answer, score, low, high, medium_action, abstain_text):\n",
    "#     if score < low:\n",
    "#         return abstain_text\n",
    "#     if score >= high:\n",
    "#         return keep_answer\n",
    "#     if medium_action == \"abstain\":\n",
    "#         return abstain_text\n",
    "#     if medium_action == \"keep\":\n",
    "#         return keep_answer\n",
    "#     return rewritten_answer\n",
    "\n",
    "\n",
    "# def apply_derec_verifier_three_zone(res, qa_dataset, texts_ds):\n",
    "#     res = res.copy()\n",
    "#     qa_dataset = qa_dataset.copy()\n",
    "#     texts_ds = texts_ds.copy()\n",
    "#     verifier_tok = AutoTokenizer.from_pretrained(DEREC_VERIFIER_REPO, token=(HF_TOKEN.strip() or None))\n",
    "#     verifier = AutoModelForSequenceClassification.from_pretrained(DEREC_VERIFIER_REPO, token=(HF_TOKEN.strip() or None))\n",
    "#     verifier.eval()\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     verifier.to(device)\n",
    "\n",
    "#     q_idx = _build_question_index(questions_ds)\n",
    "#     t_idx = _build_text_index(texts_ds)\n",
    "#     answer_map = _build_answer_map(qa_dataset)\n",
    "\n",
    "#     meta = {}\n",
    "#     with torch.no_grad():\n",
    "#         for qid, pred in res.items():\n",
    "#             qid_str = str(qid)\n",
    "#             question = q_idx.get(qid_str, {}).get(\"question\", \"\")\n",
    "#             answer = pred.get(\"model_answer\", \"\")\n",
    "#             evidence = \"\\n\".join([t_idx.get(doc_id, \"\") for doc_id in pred.get(\"found_ids\", [])])[:4000]\n",
    "#             text = f\"claim: question: {question}\\nanswer: {answer} [SEP] evidence: {evidence}\"\n",
    "#             inp = verifier_tok(text, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "#             inp = {k: v.to(device) for k, v in inp.items()}\n",
    "#             probs = torch.softmax(verifier(**inp).logits, dim=-1)[0].cpu().numpy()\n",
    "#             true_score = float(probs[2] if probs.shape[0] >= 3 else probs.max())\n",
    "#             meta[qid_str] = {\n",
    "#                 \"true_score\": true_score,\n",
    "#                 \"question\": question,\n",
    "#                 \"evidence\": evidence,\n",
    "#             }\n",
    "\n",
    "#     if DEREC_MEDIUM_ACTION == \"rewrite\":\n",
    "#         print(\"Preparing rewritten answers for medium-confidence zone...\")\n",
    "#         for qid, pred in res.items():\n",
    "#             qid_str = str(qid)\n",
    "#             meta[qid_str][\"rewritten_answer\"] = _rewrite_answer(\n",
    "#                 meta[qid_str][\"question\"],\n",
    "#                 meta[qid_str][\"evidence\"],\n",
    "#                 pred.get(\"model_answer\", \"\"),\n",
    "#             )\n",
    "\n",
    "#     low, high = DEREC_LOW_THRESHOLD, DEREC_HIGH_THRESHOLD\n",
    "#     if DEREC_CALIBRATE:\n",
    "#         low_grid = np.arange(0.15, 0.81, 0.05)\n",
    "#         high_grid = np.arange(0.35, 0.96, 0.05)\n",
    "#         dev_ids = [str(item[\"id\"]) for item in questions_ds[\"train\"][: min(DEREC_DEV_SIZE, len(questions_ds[\"train\"]))]]\n",
    "\n",
    "#         def _get_pred(qid):\n",
    "#             if qid in res:\n",
    "#                 return res[qid]\n",
    "#             try:\n",
    "#                 return res.get(int(qid))\n",
    "#             except Exception:\n",
    "#                 return None\n",
    "\n",
    "#         best_score, best_low, best_high = -1.0, low, high\n",
    "#         for l in low_grid:\n",
    "#             for h in high_grid:\n",
    "#                 if h <= l:\n",
    "#                     continue\n",
    "#                 vals = []\n",
    "#                 for qid in dev_ids:\n",
    "#                     pred = _get_pred(qid)\n",
    "#                     if pred is None or qid not in answer_map or qid not in meta:\n",
    "#                         continue\n",
    "#                     cand = _choose_answer(\n",
    "#                         keep_answer=pred.get(\"model_answer\", \"\"),\n",
    "#                         rewritten_answer=meta[qid].get(\"rewritten_answer\", pred.get(\"model_answer\", \"\")),\n",
    "#                         score=meta[qid][\"true_score\"],\n",
    "#                         low=l,\n",
    "#                         high=h,\n",
    "#                         medium_action=DEREC_MEDIUM_ACTION,\n",
    "#                         abstain_text=ABSTAIN_TEXT,\n",
    "#                     )\n",
    "#                     vals.append(_score_answer(DEREC_CALIBRATION_METRIC, cand, answer_map[qid]))\n",
    "#                 avg = float(np.mean(vals)) if vals else 0.0\n",
    "#                 if avg > best_score:\n",
    "#                     best_score, best_low, best_high = avg, float(l), float(h)\n",
    "#         low, high = best_low, best_high\n",
    "#         print(f\"Calibrated thresholds: low={low:.2f}, high={high:.2f}, {DEREC_CALIBRATION_METRIC}={best_score:.4f}\")\n",
    "\n",
    "#     kept = rewritten = abstained = 0\n",
    "#     for qid, pred in res.items():\n",
    "#         qid_str = str(qid)\n",
    "#         chosen = _choose_answer(\n",
    "#             keep_answer=pred.get(\"model_answer\", \"\"),\n",
    "#             rewritten_answer=meta[qid_str].get(\"rewritten_answer\", pred.get(\"model_answer\", \"\")),\n",
    "#             score=meta[qid_str][\"true_score\"],\n",
    "#             low=low,\n",
    "#             high=high,\n",
    "#             medium_action=DEREC_MEDIUM_ACTION,\n",
    "#             abstain_text=ABSTAIN_TEXT,\n",
    "#         )\n",
    "#         if chosen == ABSTAIN_TEXT:\n",
    "#             abstained += 1\n",
    "#         elif chosen == pred.get(\"model_answer\", \"\"):\n",
    "#             kept += 1\n",
    "#         else:\n",
    "#             rewritten += 1\n",
    "#         pred[\"model_answer\"] = chosen\n",
    "\n",
    "#     print(f\"DeReC verifier: kept={kept}, rewritten={rewritten}, abstained={abstained}, total={len(res)}\")\n",
    "#     print(f\"Effective thresholds: low={low:.2f}, high={high:.2f}\")\n",
    "#     return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544afe8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75968300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if USE_DEREC_VERIFIER and DEREC_VERIFIER_REPO.strip():\n",
    "#     res_derec_2 = apply_derec_verifier_three_zone(res, qa_dataset, texts_ds)\n",
    "# else:\n",
    "#     print(\"DeReC verifier is disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dd28239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _build_question_index(questions_ds):\n",
    "#     return {str(item[\"id\"]): item for item in questions_ds[\"train\"]}\n",
    "\n",
    "# def _build_text_index(texts_ds):\n",
    "#     return {item[\"id\"]: item[\"text\"] for item in texts_ds[\"train\"]}\n",
    "\n",
    "# def apply_derec_verifier(rag_results, questions_ds, texts_ds, repo_id, threshold=0.5, hf_token=None, max_evidence_chars=4000):\n",
    "#     rag_results = rag_results.copy()\n",
    "#     questions_ds = questions_ds.copy()\n",
    "#     texts_ds = texts_ds.copy()\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(repo_id, token=hf_token)\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(repo_id, token=hf_token)\n",
    "#     model.eval()\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     model.to(device)\n",
    "\n",
    "#     question_idx = _build_question_index(questions_ds)\n",
    "#     text_idx = _build_text_index(texts_ds)\n",
    "\n",
    "#     abstained = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for qid, pred in rag_results.items():\n",
    "#             total += 1\n",
    "#             q = question_idx.get(str(qid), {})\n",
    "#             question = q.get(\"question\", \"\")\n",
    "#             answer = pred.get(\"model_answer\", \"\")\n",
    "#             found_ids = pred.get(\"found_ids\", [])\n",
    "\n",
    "#             evidence = \"\\n\".join([text_idx.get(doc_id, \"\") for doc_id in found_ids])[:max_evidence_chars]\n",
    "#             claim = f\"question: {question}\\nanswer: {answer}\"\n",
    "#             text = f\"claim: {claim} [SEP] evidence: {evidence}\"\n",
    "\n",
    "#             inputs = tokenizer(text, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "#             inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "#             probs = torch.softmax(model(**inputs).logits, dim=-1)[0].cpu().numpy()\n",
    "#             print(probs)\n",
    "\n",
    "#             true_score = float(probs[2] if probs.shape[0] >= 3 else probs.max())\n",
    "#             print(true_score, threshold)\n",
    "#             if true_score < threshold:\n",
    "#                 pred[\"model_answer\"] = ABSTAIN_TEXT\n",
    "#                 abstained += 1\n",
    "\n",
    "#     print(f\"DeReC verifier: abstained {abstained}/{total} ({abstained/max(total,1):.2%})\")\n",
    "#     return rag_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a2a99c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0d20328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if USE_DEREC_VERIFIER and DEREC_VERIFIER_REPO.strip():\n",
    "#     res_derec = apply_derec_verifier(\n",
    "#         res, questions_ds, texts_ds,\n",
    "#         repo_id=DEREC_VERIFIER_REPO.strip(),\n",
    "#         threshold=DEREC_VERIFIER_THRESHOLD,\n",
    "#         hf_token=(HF_TOKEN.strip() or None),\n",
    "#     )\n",
    "# else:\n",
    "#     print(\"DeReC verifier is disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8362b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_derec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d680a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the baseline results\n",
    "\n",
    "res_path_derec = \"./test_derec.json\"\n",
    "results.save(res_derec, res_path_derec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "55f30dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'question': 'Какой регион был определён в качестве приоритетного для Дональда Трампа?'}\n",
      "\n",
      "{\n",
      "    \"found_ids\": [\n",
      "        197,\n",
      "        134,\n",
      "        399,\n",
      "        115,\n",
      "        458\n",
      "    ],\n",
      "    \"model_answer\": \"Индхо-Тихоокеанский регион\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "demo_public_id = 0\n",
    "print(questions_ds[\"train\"].filter(lambda it: it[\"id\"] == demo_public_id)[0])\n",
    "print('\\n' + json.dumps(res_derec_2[demo_public_id], ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f4db7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate metrics\n",
    "\n",
    "evaluation_results_derec = evaluator.evaluate_rag_results(res_derec_2, qa_dataset, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6821fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Metrics:\n",
      "+----------+---------+\n",
      "| Metric   |   Value |\n",
      "+==========+=========+\n",
      "| Hit Rate |  0.7997 |\n",
      "+----------+---------+\n",
      "| MRR      |  0.7893 |\n",
      "+----------+---------+\n",
      "\n",
      "Generation Metrics:\n",
      "+-----------------+---------+\n",
      "| Metric          |   Value |\n",
      "+=================+=========+\n",
      "| ROUGE-1         |  0.5069 |\n",
      "+-----------------+---------+\n",
      "| ROUGE-2         |  0.3159 |\n",
      "+-----------------+---------+\n",
      "| ROUGE-L         |  0.4849 |\n",
      "+-----------------+---------+\n",
      "| Exact Match     |  0.1233 |\n",
      "+-----------------+---------+\n",
      "| Substring Match |  0.1500 |\n",
      "+-----------------+---------+\n"
     ]
    }
   ],
   "source": [
    "_ = evaluation_results.to_table(overall_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "67b33b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Metrics:\n",
      "+----------+---------+\n",
      "| Metric   |   Value |\n",
      "+==========+=========+\n",
      "| Hit Rate |  0.7997 |\n",
      "+----------+---------+\n",
      "| MRR      |  0.7893 |\n",
      "+----------+---------+\n",
      "\n",
      "Generation Metrics:\n",
      "+-----------------+---------+\n",
      "| Metric          |   Value |\n",
      "+=================+=========+\n",
      "| ROUGE-1         |  0.4903 |\n",
      "+-----------------+---------+\n",
      "| ROUGE-2         |  0.2945 |\n",
      "+-----------------+---------+\n",
      "| ROUGE-L         |  0.4675 |\n",
      "+-----------------+---------+\n",
      "| Exact Match     |  0.1183 |\n",
      "+-----------------+---------+\n",
      "| Substring Match |  0.1450 |\n",
      "+-----------------+---------+\n"
     ]
    }
   ],
   "source": [
    "_ = evaluation_results_derec.to_table(overall_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15d6eb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Metrics:\n",
      "+----------+-----------+--------+--------+--------+----------+\n",
      "| Metric   |   Overall |   cond |     mh |    set |   simple |\n",
      "+==========+===========+========+========+========+==========+\n",
      "| Hit Rate |    0.7997 | 0.7733 | 0.7756 | 0.8033 |   0.8467 |\n",
      "+----------+-----------+--------+--------+--------+----------+\n",
      "| MRR      |    0.7893 | 0.7999 | 0.7730 | 0.7841 |   0.8000 |\n",
      "+----------+-----------+--------+--------+--------+----------+\n",
      "\n",
      "Generation Metrics:\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| Metric          |   Overall |   cond |     mh |    set |   simple |\n",
      "+=================+===========+========+========+========+==========+\n",
      "| ROUGE-1         |    0.4903 | 0.5921 | 0.5155 | 0.4416 |   0.4121 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| ROUGE-2         |    0.2945 | 0.3613 | 0.2951 | 0.2198 |   0.3016 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| ROUGE-L         |    0.4675 | 0.5908 | 0.5148 | 0.3541 |   0.4104 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| Exact Match     |    0.1183 | 0.2067 | 0.1800 | 0.0000 |   0.0867 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| Substring Match |    0.1450 | 0.2600 | 0.1933 | 0.0267 |   0.1000 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "_ = evaluation_results_derec.to_table(overall_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "79416186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Metrics:\n",
      "+----------+-----------+--------+--------+--------+----------+\n",
      "| Metric   |   Overall |   cond |     mh |    set |   simple |\n",
      "+==========+===========+========+========+========+==========+\n",
      "| Hit Rate |    0.7997 | 0.7733 | 0.7756 | 0.8033 |   0.8467 |\n",
      "+----------+-----------+--------+--------+--------+----------+\n",
      "| MRR      |    0.7893 | 0.7999 | 0.7730 | 0.7841 |   0.8000 |\n",
      "+----------+-----------+--------+--------+--------+----------+\n",
      "\n",
      "Generation Metrics:\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| Metric          |   Overall |   cond |     mh |    set |   simple |\n",
      "+=================+===========+========+========+========+==========+\n",
      "| ROUGE-1         |    0.5069 | 0.6177 | 0.4885 | 0.4278 |   0.4936 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| ROUGE-2         |    0.3159 | 0.3795 | 0.2877 | 0.2234 |   0.3731 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| ROUGE-L         |    0.4849 | 0.6144 | 0.4885 | 0.3432 |   0.4936 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| Exact Match     |    0.1233 | 0.2533 | 0.1267 | 0.0000 |   0.1133 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n",
      "| Substring Match |    0.1500 | 0.2933 | 0.1333 | 0.0333 |   0.1400 |\n",
      "+-----------------+-----------+--------+--------+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "_ = evaluation_results.to_table(overall_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8fa311",
   "metadata": {},
   "source": [
    "## Top-10 Documents By Fact-Check Score\n",
    "Вычисляет score класса `true` (DeReC verifier) для каждого ответа и агрегирует его по найденным документам (`found_ids`).\n",
    "Выводит top-10 документов по среднему score и количеству появлений.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f0481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "TOP_N_DOCS = 10\n",
    "\n",
    "if not (DEREC_VERIFIER_REPO and DEREC_VERIFIER_REPO.strip()):\n",
    "    print(\"Set DEREC_VERIFIER_REPO first (cell 'DeReC Verifier (optional)').\")\n",
    "else:\n",
    "    verifier_tok = AutoTokenizer.from_pretrained(DEREC_VERIFIER_REPO, token=(HF_TOKEN.strip() or None))\n",
    "    verifier = AutoModelForSequenceClassification.from_pretrained(DEREC_VERIFIER_REPO, token=(HF_TOKEN.strip() or None))\n",
    "    verifier.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    verifier.to(device)\n",
    "\n",
    "    q_idx = _build_question_index(questions_ds)\n",
    "    t_idx = _build_text_index(texts_ds)\n",
    "\n",
    "    doc_score_sum = defaultdict(float)\n",
    "    doc_score_cnt = defaultdict(int)\n",
    "    doc_score_max = defaultdict(float)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for qid, pred in res.items():\n",
    "            qid_str = str(qid)\n",
    "            question = q_idx.get(qid_str, {}).get(\"question\", \"\")\n",
    "            answer = pred.get(\"model_answer\", \"\")\n",
    "            found_ids = pred.get(\"found_ids\", [])\n",
    "            evidence = \"\\n\".join([t_idx.get(doc_id, \"\") for doc_id in found_ids])[:4000]\n",
    "\n",
    "            text = f\"claim: question: {question}\\nanswer: {answer} [SEP] evidence: {evidence}\"\n",
    "            inp = verifier_tok(text, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "            inp = {k: v.to(device) for k, v in inp.items()}\n",
    "            probs = torch.softmax(verifier(**inp).logits, dim=-1)[0].cpu().numpy()\n",
    "            true_score = float(probs[2] if probs.shape[0] >= 3 else probs.max())\n",
    "\n",
    "            for doc_id in found_ids:\n",
    "                doc_score_sum[doc_id] += true_score\n",
    "                doc_score_cnt[doc_id] += 1\n",
    "                doc_score_max[doc_id] = max(doc_score_max[doc_id], true_score)\n",
    "\n",
    "    ranked = []\n",
    "    for doc_id, total in doc_score_sum.items():\n",
    "        cnt = doc_score_cnt[doc_id]\n",
    "        avg = total / cnt\n",
    "        ranked.append((doc_id, avg, cnt, doc_score_max[doc_id]))\n",
    "\n",
    "    ranked.sort(key=lambda x: (-x[1], -x[2], -x[3]))\n",
    "    top_docs = ranked[:TOP_N_DOCS]\n",
    "\n",
    "    print(f\"Top-{TOP_N_DOCS} documents by average fact-check true_score\")\n",
    "    for i, (doc_id, avg_score, cnt, max_score) in enumerate(top_docs, 1):\n",
    "        print(f\"{i:02d}. doc_id={doc_id} | avg_score={avg_score:.4f} | occurrences={cnt} | max_score={max_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
