{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate DeReC Fact-Checking Verifier\n",
    "\n",
    "DRAGON has no explicit \"grounded/ungrounded\" labels, so we create a synthetic evaluation dataset:\n",
    "- **Grounded (0-99)**: answers via normal RAG pipeline (label=1)\n",
    "- **Ungrounded (100-199)**: LLM answers WITHOUT context (label=0)\n",
    "\n",
    "Then run DeReC verifier and measure its ability to distinguish grounded from ungrounded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/BigMak1/rag_fact_checking.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r rag_fact_checking/DRAGON/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"ERROR\"\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from langchain_community.llms import VLLM\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from rag_bench import baseline, data, evaluator, results\n",
    "from rag_fact_checking.DRAGON.rag_bench import baseline, data, evaluator, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Constants ──\n",
    "N_GROUNDED = 100\n",
    "N_UNGROUNDED = 100\n",
    "\n",
    "HIST_PRIVATE_QA_REPO_ID: str = \"ai-forever/hist-rag-bench-private-qa\"\n",
    "HIST_PRIVATE_TEXTS_REPO_ID: str = \"ai-forever/hist-rag-bench-private-texts\"\n",
    "RANDOM_SEED: int = 42\n",
    "EMBEDDER_NAME: str = \"ai-forever/FRIDA\"\n",
    "LLM_NAME: str = \"bond005/meno-tiny-0.1\"\n",
    "\n",
    "DEREC_VERIFIER_REPO: str = \"\"  # e.g. \"evilfreelancer/ruRoBERTa-DeReC-v1\"\n",
    "HF_TOKEN: str = \"\"             # set if the verifier repo is private\n",
    "\n",
    "# ── Prompts ──\n",
    "LLM_PROMPT: str = \"\"\"Проанализируйте заданный контекст и ответьте на вопрос пользователя на основе сведений, предоставленных в этом контексте.\n",
    "Не давайте никаких объяснений и пояснений к своему ответу. Не пишите ничего лишнего. Не извиняйтесь, не стройте диалог. Выдавайте только ответ и ничего больше.\n",
    "Отвечайте на русском языке.\n",
    "Если в заданном контексте нет информации для ответа на вопрос пользователя, то ничего не придумывайте и просто откажитесь отвечать.\n",
    "\"\"\"\n",
    "\n",
    "LLM_PROMPT_NO_CONTEXT: str = \"\"\"Ответьте на вопрос пользователя.\n",
    "Не давайте никаких объяснений. Выдавайте только ответ и ничего больше.\n",
    "Отвечайте на русском языке.\n",
    "\"\"\"\n",
    "\n",
    "# ── Helpers ──\n",
    "def _build_question_index(questions_ds):\n",
    "    \"\"\"Map str(question_id) -> {\"question\": ...}\"\"\"\n",
    "    idx = {}\n",
    "    for item in questions_ds[\"train\"]:\n",
    "        idx[str(item[\"id\"])] = {\"question\": item[\"question\"]}\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _build_text_index(texts_ds):\n",
    "    \"\"\"Map doc_id -> text content\"\"\"\n",
    "    idx = {}\n",
    "    for item in texts_ds[\"train\"]:\n",
    "        idx[item[\"id\"]] = item[\"text\"]\n",
    "    return idx\n",
    "\n",
    "\n",
    "def get_private_qa_dataset(version):\n",
    "    return load_dataset(HIST_PRIVATE_QA_REPO_ID, revision=version)\n",
    "\n",
    "\n",
    "def get_private_texts_dataset(version):\n",
    "    return load_dataset(HIST_PRIVATE_TEXTS_REPO_ID, revision=version)\n",
    "\n",
    "\n",
    "def get_public_to_private_texts_mapping(version):\n",
    "    private_texts_ds = get_private_texts_dataset(version)\n",
    "    mapping = {}\n",
    "    for item in private_texts_ds[\"train\"]:\n",
    "        mapping[item[\"public_id\"]] = item[\"id\"]\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.random.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ds, questions_ds, version = data.get_datasets(is_hist=True)\n",
    "print(f\"version = {version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = get_private_qa_dataset(version)\n",
    "mapping = get_public_to_private_texts_mapping(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle FIX\n",
    "os.environ[\"LIBRARY_PATH\"] = \"/usr/local/nvidia/lib64:\" + os.environ.get(\"LIBRARY_PATH\", \"\")\n",
    "print(\"LIBRARY_PATH =\", os.environ[\"LIBRARY_PATH\"])\n",
    "\n",
    "llm = VLLM(\n",
    "    model=LLM_NAME,\n",
    "    tensor_parallel_size=2,\n",
    "    max_new_tokens=256,\n",
    "    top_p=0.95,\n",
    "    temperature=0.3,\n",
    "    vllm_kwargs={\n",
    "        \"gpu_memory_utilization\": 0.45,\n",
    "        \"max_num_batched_tokens\": 8192,\n",
    "        \"max_model_len\": 4096,\n",
    "        \"disable_log_stats\": True,\n",
    "        \"seed\": RANDOM_SEED\n",
    "    },\n",
    "    disable_log_stats=True,\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(LLM_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDER_NAME,\n",
    "    model_kwargs={\"trust_remote_code\": True},\n",
    "    encode_kwargs={\"batch_size\": 16, \"prompt\": \"search_document: \"},\n",
    "    query_encode_kwargs={\"prompt\": \"search_query: \"}\n",
    ")\n",
    "\n",
    "retrieval = baseline.init_retriever(\n",
    "    texts_ds,\n",
    "    embedding_model,\n",
    "    top_k=5,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "generation = baseline.init_generation(retrieval, llm, tok, system_prompt=LLM_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grounded Answers (questions 0-99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "\n",
    "res_grounded = baseline.get_results(\n",
    "    generation, questions_ds, take=N_GROUNDED, write_logs=False\n",
    ")\n",
    "print(f\"Grounded answers: {len(res_grounded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ungrounded Answers (questions 100-199)\n",
    "\n",
    "For each question:\n",
    "1. Run retriever to get documents (needed for DeReC evidence)\n",
    "2. Generate answer with LLM **without context** — purely from parametric knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Build no-context prompt template\n",
    "messages_no_ctx = [\n",
    "    {\"role\": \"system\", \"content\": LLM_PROMPT_NO_CONTEXT},\n",
    "    {\"role\": \"user\", \"content\": \"Вопрос: {question}\"},\n",
    "]\n",
    "template_no_ctx = tok.apply_chat_template(\n",
    "    messages_no_ctx, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "res_ungrounded = {}\n",
    "ungrounded_slice = questions_ds[\"train\"].select(range(N_GROUNDED, N_GROUNDED + N_UNGROUNDED))\n",
    "\n",
    "for item in tqdm(ungrounded_slice, desc=\"Ungrounded\"):\n",
    "    # Retrieve documents (for DeReC evidence later)\n",
    "    try:\n",
    "        docs = retrieval.invoke(item[\"question\"])\n",
    "    except AttributeError:\n",
    "        docs = retrieval.get_relevant_documents(item[\"question\"])\n",
    "\n",
    "    # Generate answer WITHOUT context\n",
    "    prompt = template_no_ctx.replace(\"{question}\", item[\"question\"])\n",
    "    answer = llm.invoke(prompt)\n",
    "\n",
    "    res_ungrounded[item[\"id\"]] = {\n",
    "        \"found_ids\": [d.metadata[\"id\"] for d in docs],\n",
    "        \"model_answer\": answer,\n",
    "    }\n",
    "\n",
    "print(f\"Ungrounded answers: {len(res_ungrounded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine Results + Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {**res_grounded, **res_ungrounded}\n",
    "assert len(res) == N_GROUNDED + N_UNGROUNDED, f\"Expected {N_GROUNDED + N_UNGROUNDED}, got {len(res)}\"\n",
    "\n",
    "labels = {}\n",
    "for qid in res_grounded:\n",
    "    labels[qid] = True   # grounded\n",
    "for qid in res_ungrounded:\n",
    "    labels[qid] = False  # ungrounded\n",
    "\n",
    "results.save(res, \"./fact_check_eval_results.json\")\n",
    "print(f\"Saved {len(res)} results. Grounded: {sum(labels.values())}, Ungrounded: {sum(not v for v in labels.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DRAGON Metrics by Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounded_ids = set(res_grounded.keys())\n",
    "ungrounded_ids = set(res_ungrounded.keys())\n",
    "\n",
    "# Filter private qa_dataset to the relevant public_ids for each group\n",
    "qa_grounded = qa_dataset[\"train\"].filter(lambda x: x[\"public_id\"] in grounded_ids)\n",
    "qa_ungrounded = qa_dataset[\"train\"].filter(lambda x: x[\"public_id\"] in ungrounded_ids)\n",
    "qa_grounded_dd = DatasetDict({\"train\": qa_grounded})\n",
    "qa_ungrounded_dd = DatasetDict({\"train\": qa_ungrounded})\n",
    "\n",
    "print(f\"=== GROUNDED (n={len(qa_grounded)}) ===\")\n",
    "eval_grounded = evaluator.evaluate_rag_results(res_grounded, qa_grounded_dd, mapping)\n",
    "_ = eval_grounded.to_table(overall_only=True)\n",
    "\n",
    "print(f\"\\n=== UNGROUNDED (n={len(qa_ungrounded)}) ===\")\n",
    "eval_ungrounded = evaluator.evaluate_rag_results(res_ungrounded, qa_ungrounded_dd, mapping)\n",
    "_ = eval_ungrounded.to_table(overall_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DeReC Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory from LLM before loading verifier\n",
    "del llm\n",
    "torch.cuda.empty_cache()\n",
    "print(\"LLM removed, GPU memory freed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert DEREC_VERIFIER_REPO.strip(), \"Set DEREC_VERIFIER_REPO in the constants cell first!\"\n",
    "\n",
    "hf_token = HF_TOKEN.strip() or None\n",
    "verifier_tok = AutoTokenizer.from_pretrained(DEREC_VERIFIER_REPO, token=hf_token)\n",
    "verifier = AutoModelForSequenceClassification.from_pretrained(DEREC_VERIFIER_REPO, token=hf_token)\n",
    "verifier.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "verifier.to(device)\n",
    "\n",
    "q_idx = _build_question_index(questions_ds)\n",
    "t_idx = _build_text_index(texts_ds)\n",
    "\n",
    "# Score every result\n",
    "derec_scores = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for qid, pred in tqdm(res.items(), desc=\"DeReC scoring\"):\n",
    "        question = q_idx.get(str(qid), {}).get(\"question\", \"\")\n",
    "        answer = pred.get(\"model_answer\", \"\")\n",
    "        found_ids = pred.get(\"found_ids\", [])\n",
    "        evidence = \"\\n\".join([t_idx.get(doc_id, \"\") for doc_id in found_ids])[:4000]\n",
    "\n",
    "        text = f\"claim: question: {question}\\nanswer: {answer} [SEP] evidence: {evidence}\"\n",
    "        inp = verifier_tok(\n",
    "            text, max_length=512, padding=\"max_length\",\n",
    "            truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        inp = {k: v.to(device) for k, v in inp.items()}\n",
    "        probs = torch.softmax(verifier(**inp).logits, dim=-1)[0].cpu().numpy()\n",
    "        true_score = float(probs[2] if probs.shape[0] >= 3 else probs.max())\n",
    "        derec_scores[qid] = true_score\n",
    "\n",
    "# Statistics by group\n",
    "scores_grounded = [derec_scores[qid] for qid in grounded_ids]\n",
    "scores_ungrounded = [derec_scores[qid] for qid in ungrounded_ids]\n",
    "\n",
    "print(f\"\\nGrounded   — mean: {np.mean(scores_grounded):.4f}, median: {np.median(scores_grounded):.4f}, std: {np.std(scores_grounded):.4f}\")\n",
    "print(f\"Ungrounded — mean: {np.mean(scores_ungrounded):.4f}, median: {np.median(scores_ungrounded):.4f}, std: {np.std(scores_ungrounded):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build arrays aligned by qid\n",
    "qids = sorted(res.keys())\n",
    "y_true = np.array([1 if labels[qid] else 0 for qid in qids])\n",
    "y_scores = np.array([derec_scores[qid] for qid in qids])\n",
    "\n",
    "# ROC-AUC\n",
    "auc = roc_auc_score(y_true, y_scores)\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "\n",
    "# Optimal threshold (Youden's J)\n",
    "j_scores = tpr - fpr\n",
    "best_idx = np.argmax(j_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_tpr = tpr[best_idx]\n",
    "best_fpr = fpr[best_idx]\n",
    "\n",
    "print(f\"ROC-AUC: {auc:.4f}\")\n",
    "print(f\"Optimal threshold (Youden's J): {best_threshold:.4f}\")\n",
    "print(f\"  TPR: {best_tpr:.4f}, FPR: {best_fpr:.4f}\")\n",
    "\n",
    "# Metrics at several thresholds\n",
    "print(\"\\n\" + \"=\"*65)\n",
    "print(f\"{'Threshold':>10} {'TPR':>8} {'FPR':>8} {'Precision':>10} {'F1':>8}\")\n",
    "print(\"=\"*65)\n",
    "for t in [0.3, 0.4, 0.5, best_threshold, 0.6, 0.7, 0.8]:\n",
    "    y_pred = (y_scores >= t).astype(int)\n",
    "    tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "    fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "    fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "    tpr_t = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fpr_t = fp / (fp + (y_true == 0).sum()) if (y_true == 0).sum() > 0 else 0\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1 = 2 * prec * tpr_t / (prec + tpr_t) if (prec + tpr_t) > 0 else 0\n",
    "    marker = \" *\" if abs(t - best_threshold) < 1e-6 else \"\"\n",
    "    print(f\"{t:>10.4f} {tpr_t:>8.4f} {fpr_t:>8.4f} {prec:>10.4f} {f1:>8.4f}{marker}\")\n",
    "print(\"=\"*65)\n",
    "print(\"* = optimal (Youden's J)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram: DeReC score distribution for grounded vs ungrounded\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(scores_grounded, bins=25, alpha=0.6, label=\"Grounded\", color=\"steelblue\")\n",
    "ax.hist(scores_ungrounded, bins=25, alpha=0.6, label=\"Ungrounded\", color=\"tomato\")\n",
    "ax.axvline(best_threshold, color=\"black\", linestyle=\"--\", label=f\"Threshold={best_threshold:.3f}\")\n",
    "ax.set_xlabel(\"DeReC true_score\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"DeReC Score Distribution: Grounded vs Ungrounded\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.plot(fpr, tpr, color=\"steelblue\", lw=2, label=f\"ROC (AUC={auc:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\", lw=1, label=\"Random\")\n",
    "ax.scatter([best_fpr], [best_tpr], color=\"red\", s=100, zorder=5,\n",
    "           label=f\"Optimal (t={best_threshold:.3f})\")\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(\"ROC Curve — DeReC as Groundedness Detector\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_xlim(-0.02, 1.02)\n",
    "ax.set_ylim(-0.02, 1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter: ROUGE-L vs DeReC score for ungrounded answers\n",
    "# Shows parametric knowledge cases: high ROUGE-L + should be low DeReC\n",
    "rouge_l_ungrounded = []\n",
    "derec_ungrounded = []\n",
    "\n",
    "for qid in ungrounded_ids:\n",
    "    public_id_str = str(qid)\n",
    "    if public_id_str in eval_ungrounded.individual_results:\n",
    "        rouge_l = eval_ungrounded.individual_results[public_id_str][\"generation\"][\"rougeL\"]\n",
    "    elif qid in eval_ungrounded.individual_results:\n",
    "        rouge_l = eval_ungrounded.individual_results[qid][\"generation\"][\"rougeL\"]\n",
    "    else:\n",
    "        continue\n",
    "    rouge_l_ungrounded.append(rouge_l)\n",
    "    derec_ungrounded.append(derec_scores[qid])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "sc = ax.scatter(rouge_l_ungrounded, derec_ungrounded, alpha=0.6, c=\"tomato\", edgecolors=\"gray\", s=40)\n",
    "ax.axhline(best_threshold, color=\"black\", linestyle=\"--\", alpha=0.7, label=f\"DeReC threshold={best_threshold:.3f}\")\n",
    "ax.set_xlabel(\"ROUGE-L (ungrounded vs reference)\")\n",
    "ax.set_ylabel(\"DeReC true_score\")\n",
    "ax.set_title(\"Ungrounded: ROUGE-L vs DeReC Score\\n(top-right = parametric knowledge)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count parametric knowledge cases\n",
    "n_high_rouge = sum(1 for r in rouge_l_ungrounded if r > 0.5)\n",
    "n_high_rouge_high_derec = sum(\n",
    "    1 for r, d in zip(rouge_l_ungrounded, derec_ungrounded) if r > 0.5 and d > best_threshold\n",
    ")\n",
    "print(f\"Ungrounded with ROUGE-L > 0.5 (parametric knowledge): {n_high_rouge}/{len(rouge_l_ungrounded)}\")\n",
    "print(f\"  of which DeReC > threshold (false positives): {n_high_rouge_high_derec}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}